{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from itertools import combinations \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_name = sys.argv[1]\n",
    "# support = int(sys.argv[2])\n",
    "# confidence = int(sys.argv[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'associationruletestdata.txt'\n",
    "support = 50\n",
    "confidence = 70\n",
    "\n",
    "data = pd.read_csv(file_name, sep='\\t', lineterminator='\\n', header=None)\n",
    "unique_item_set = set()\n",
    "i = 0\n",
    "for column in data.columns[:-1]:\n",
    "    data[column] = 'G' + str(column+1) + \"_\" + data[column].astype(str)\n",
    "    unique_item_set = unique_item_set | set(data[column].unique())\n",
    "\n",
    "unique_item_set = unique_item_set | set(data.iloc[:,-1])\n",
    "\n",
    "data_set = []\n",
    "for row_index in range(len(data)):\n",
    "    data_set.append(set(data.iloc[row_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "support = (support/100)*len(data)\n",
    "confidence = confidence/100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "association_rules = pd.DataFrame(columns=['RULE','HEAD','BODY','SUPPORT','CONFIDENCE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frequent_list_map = {}\n",
    "max_length_frequent_itemsets = 0\n",
    "\n",
    "frequent_itemset = set()\n",
    "for unique_item in unique_item_set:\n",
    "    current_item_count = 0\n",
    "    for data_item in data_set:\n",
    "        current_item_count = (current_item_count+1) if(unique_item in data_item) else int(current_item_count) \n",
    "        if (current_item_count<support):\n",
    "            continue\n",
    "        frequent_itemset.add(str(unique_item))\n",
    "        list_of_items = list()\n",
    "        list_of_items.append(str(unique_item))\n",
    "#         print(set(list_of_items))\n",
    "        all_frequent_list_map[str(set(list_of_items))] = current_item_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of length-1 frequent itemsets: 109\n",
      "number of length-2 frequent itemsets: 63\n",
      "number of length-3 frequent itemsets: 2\n",
      "117 rules are generated. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "no_of_data_columns = len(data.columns)\n",
    "current_frequent_list = list(map(lambda elem:[elem], list(frequent_itemset))) \n",
    "current_frequent_list.sort()\n",
    "visited_rules = []\n",
    "print(\"number of length-1 frequent itemsets: \"+str(len(frequent_itemset)))\n",
    "\n",
    "\n",
    "max_length_frequent_itemsets = 0\n",
    "for i in range(len(frequent_itemset)):\n",
    "    new_frequent_list = []\n",
    "    for j in range(len(current_frequent_list)):\n",
    "        for k in range(j+1, len(current_frequent_list)):\n",
    "            merged_list = list(set(current_frequent_list[j]).union(set(current_frequent_list[k])))\n",
    "            if (len(merged_list) == len(current_frequent_list[j])+1):\n",
    "                temp_merged_list = list(merged_list)\n",
    "                temp_merged_list.sort()\n",
    "                subsets_of_merged_list = [set(item) for item in list(combinations(set(temp_merged_list), len(current_frequent_list[j])))]\n",
    "                current_item_count = 0\n",
    "                \n",
    "                for item in data_set:\n",
    "                    current_item_count = (current_item_count+1) if(set(merged_list).issubset(item)) else int(current_item_count) \n",
    "                all_frequent_list_map[str(set(temp_merged_list))] = current_item_count\n",
    "                if (current_item_count<support):\n",
    "                    continue\n",
    "                subset_count = 0\n",
    "                for current_subset_item in subsets_of_merged_list:\n",
    "                    for current_frequent_item in current_frequent_list:\n",
    "                        current_frequent_set = set(current_frequent_item)\n",
    "                        if (current_subset_item.issubset(current_frequent_set)):\n",
    "                            subset_count += 1\n",
    "                            break\n",
    "                    if subset_count == len(subsets_of_merged_list):\n",
    "                        break\n",
    "                if (subset_count == len(subsets_of_merged_list)):\n",
    "                    concatenated_list = ''.join(temp_merged_list)\n",
    "                    if (concatenated_list in visited_rules):\n",
    "                        continue\n",
    "                    visited_rules.append(concatenated_list)\n",
    "                    new_frequent_list.append(temp_merged_list)\n",
    "                    pruned_rules = []\n",
    "                    for merged_list_index in reversed(range(len(temp_merged_list))):\n",
    "                        if(len(temp_merged_list)<=(merged_list_index+1)):\n",
    "                            continue\n",
    "                        data_combinations = [set(item) for item in list(combinations(temp_merged_list,merged_list_index+1))]\n",
    "                        for current_subset_item in data_combinations:\n",
    "                            y=list(current_subset_item)\n",
    "                            y.sort()\n",
    "                            prune_list_concatenation = ''.join(y)\n",
    "                            low_confidence = False\n",
    "                            for pruned_rule in pruned_rules:\n",
    "                                if prune_list_concatenation in pruned_rule:\n",
    "                                    low_confidence = True\n",
    "                            if low_confidence:\n",
    "                                break\n",
    "                            current_confidence=all_frequent_list_map[str(set(temp_merged_list))]/all_frequent_list_map[str(set(y))]\n",
    "                            if(current_confidence>confidence):\n",
    "                                association_rules.loc[len(association_rules)]=pd.Series({'RULE': str(temp_merged_list), 'HEAD': str(current_subset_item), 'BODY': str(set(temp_merged_list).difference(current_subset_item)), 'SUPPORT': all_frequent_list_map[str(set(temp_merged_list))]/len(data), 'CONFIDENCE': current_confidence})\n",
    "                            else:\n",
    "                                pruned_rules.append(prune_list_concatenation)\n",
    "    if (len(new_frequent_list) == 0):\n",
    "        break\n",
    "    new_frequent_list = list(map(list, set(map(frozenset, new_frequent_list))))\n",
    "    current_frequent_list = new_frequent_list\n",
    "    print(\"number of length-\"+str(i+2),\"frequent itemsets: \"+str(len(current_frequent_list)))\n",
    "    max_length_frequent_itemsets = i+2    \n",
    "\n",
    "association_rules.drop_duplicates()\n",
    "# print('time taken to get frequent itemsets: ', (time.time() - start_time), ' seconds')\n",
    "print(str(len(association_rules)) + \" rules are generated. \\n\")\n",
    "association_rules.to_csv('AssociationRules.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "asso_rule.template1(\"RULE\", \"ANY\", ['G59_Up'])\n",
      "26 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template1(\"RULE\", \"NONE\", ['G59_Up'])\n",
      "91 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template1(\"RULE\", 1, ['G59_Up', 'G10_Down'])\n",
      "39 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template1(\"HEAD\", \"ANY\", ['G59_Up'])\n",
      "9 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template1(\"HEAD\", \"NONE\", ['G59_Up'])\n",
      "108 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template1(\"HEAD\", 1, ['G59_Up', 'G10_Down'])\n",
      "17 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template1(\"BODY\", \"ANY\", ['G59_Up'])\n",
      "17 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template1(\"BODY\", \"NONE\", ['G59_Up'])\n",
      "100 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template1(\"BODY\", 1, ['G59_Up', 'G10_Down'])\n",
      "24 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template2(\"RULE\", 3)\n",
      "9 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template2(\"HEAD\", 2)\n",
      "6 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template2(\"BODY\", 1)\n",
      "117 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template3(\"1or1\", \"HEAD\", \"ANY\", ['G10_Down'], \"BODY\", 1, ['G59_Up'])\n",
      "24 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template3(\"1and1\", \"HEAD\", \"ANY\", ['G10_Down'], \"BODY\", 1, ['G59_Up'])\n",
      "1 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template3(\"1or2\", \"HEAD\", \"ANY\", ['G10_Down'], \"BODY\", 2)\n",
      "11 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template3(\"1and2\", \"HEAD\", \"ANY\", ['G10_Down'], \"BODY\", 2)\n",
      "0 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template3(\"2or2\", \"HEAD\", 1, \"BODY\", 2)\n",
      "117 rules are generated for the given query.\n",
      "\n",
      "asso_rule.template3(\"2and2\", \"HEAD\", 1, \"BODY\", 2)\n",
      "3 rules are generated for the given query.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def queryTemplate1(query, result):\n",
    "    if (query[0] == 'RULE' and query[1] == 'ANY'):\n",
    "        for query_item in query[2]:\n",
    "            result = result.append(association_rules[association_rules['RULE'].str.contains(query_item)])\n",
    "    elif (query[0] == 'RULE' and query[1] == 'NONE'):\n",
    "        for query_item in query[2]:\n",
    "            result = result.append(association_rules[~association_rules['RULE'].str.contains(query_item)])\n",
    "    elif (query[0] == 'RULE' and query[1] >= 1):\n",
    "        length_combinations = [set(item) for item in list(combinations(set(query[2]),query[1]))]\n",
    "        for combination in length_combinations:\n",
    "            temp_result = pd.DataFrame(data=None, columns=association_rules.columns)\n",
    "            combination_list = list(combination)\n",
    "            positive_result = association_rules['RULE'].str.contains(combination_list[0])\n",
    "            for i in range(1, len(combination_list)):\n",
    "                positive_result = positive_result & association_rules['RULE'].str.contains(combination_list[i]) \n",
    "            temp_result = temp_result.append(association_rules[positive_result])\n",
    "            remaining_combination = set(query[2]).difference(combination)\n",
    "            remaining_combination = list(remaining_combination)\n",
    "            if (len(remaining_combination) < 1):\n",
    "                result = result.append(temp_result)\n",
    "                continue\n",
    "            negative_result = ~temp_result['RULE'].str.contains(remaining_combination[0])\n",
    "            for i in range(1, len(remaining_combination)):\n",
    "                negative_result = negative_result & ~temp_result['RULE'].str.contains(remaining_combination[i])\n",
    "            temp_result = temp_result[negative_result]\n",
    "            result = result.append(temp_result)\n",
    "    elif (query[0] == 'HEAD' and query[1] == 'ANY'):\n",
    "        for query_item in query[2]:\n",
    "            result = result.append(association_rules[association_rules['HEAD'].str.contains(query_item)])\n",
    "    elif (query[0] == 'HEAD' and query[1] == 'NONE'):\n",
    "        for query_item in query[2]:\n",
    "            result = result.append(association_rules[~association_rules['HEAD'].str.contains(query_item)])\n",
    "    elif (query[0] == 'HEAD' and query[1] >= 1):\n",
    "        length_combinations = [set(item) for item in list(combinations(set(query[2]),query[1]))]\n",
    "        for combination in length_combinations:\n",
    "            temp_result = pd.DataFrame(data=None, columns=association_rules.columns)\n",
    "            combination_list = list(combination)\n",
    "            positive_result = association_rules['HEAD'].str.contains(combination_list[0])\n",
    "            for i in range(1, len(combination_list)):\n",
    "                positive_result = positive_result & association_rules['HEAD'].str.contains(combination_list[i]) \n",
    "            temp_result = temp_result.append(association_rules[positive_result])\n",
    "            remaining_combination = set(query[2]).difference(combination)\n",
    "            remaining_combination = list(remaining_combination)\n",
    "            if (len(remaining_combination) < 1):\n",
    "                result = result.append(temp_result)\n",
    "                continue\n",
    "            negative_result = ~temp_result['HEAD'].str.contains(remaining_combination[0])\n",
    "            for i in range(1, len(remaining_combination)):\n",
    "                negative_result = negative_result & ~temp_result['HEAD'].str.contains(remaining_combination[i])\n",
    "            temp_result = temp_result[negative_result]\n",
    "            result = result.append(temp_result)\n",
    "    elif (query[0] == 'BODY' and query[1] == 'ANY'):\n",
    "        for query_item in query[2]:\n",
    "            result = result.append(association_rules[association_rules['BODY'].str.contains(query_item)])\n",
    "    elif (query[0] == 'BODY' and query[1] == 'NONE'):\n",
    "        for query_item in query[2]:\n",
    "            result = result.append(association_rules[~association_rules['BODY'].str.contains(query_item)])\n",
    "    elif (query[0] == 'BODY' and query[1] == 1):\n",
    "        length_combinations = [set(item) for item in list(combinations(set(query[2]),query[1]))]\n",
    "        for combination in length_combinations:\n",
    "            temp_result = pd.DataFrame(data=None, columns=association_rules.columns)\n",
    "            combination_list = list(combination)\n",
    "            positive_result = association_rules['BODY'].str.contains(combination_list[0])\n",
    "            for i in range(1, len(combination_list)):\n",
    "                positive_result = positive_result & association_rules['BODY'].str.contains(combination_list[i]) \n",
    "            temp_result = temp_result.append(association_rules[positive_result])\n",
    "            remaining_combination = set(query[2]).difference(combination)\n",
    "            remaining_combination = list(remaining_combination)\n",
    "            if (len(remaining_combination) < 1):\n",
    "                result = result.append(temp_result)\n",
    "                continue\n",
    "            negative_result = ~temp_result['BODY'].str.contains(remaining_combination[0])\n",
    "            for i in range(1, len(remaining_combination)):\n",
    "                negative_result = negative_result & ~temp_result['BODY'].str.contains(remaining_combination[i])\n",
    "            temp_result = temp_result[negative_result]\n",
    "            result = result.append(temp_result)\n",
    "    return result\n",
    "\n",
    "def queryTemplate2(query, result):\n",
    "    if (query[0] == 'RULE'):\n",
    "        result = result.append(association_rules[association_rules['RULE'].str.count(',')+1>=query[1]])\n",
    "    elif (query[0] == 'HEAD'):\n",
    "        result = result.append(association_rules[association_rules['HEAD'].str.count(',')+1>=query[1]])\n",
    "    elif (query[0] == 'BODY'):\n",
    "        result = result.append(association_rules[association_rules['BODY'].str.count(',')+1>=query[1]])\n",
    "    return result\n",
    "\n",
    "def queryTemplate3(query, result):\n",
    "    if (query[0] == '1or1'):\n",
    "        result = result.append(queryTemplate1(query[1:4], result))\n",
    "        result = result.append(queryTemplate1(query[4:7], result))\n",
    "    elif (query[0] == '1and1'):\n",
    "        result = pd.merge(queryTemplate1(query[1:4], result), queryTemplate1(query[4:7], result), how='inner')\n",
    "    elif (query[0] == '1or2'):\n",
    "        result = result.append(queryTemplate1(query[1:4], result))\n",
    "        result = result.append(queryTemplate2(query[4:6], result))\n",
    "    elif (query[0] == '1and2'):\n",
    "        result = pd.merge(queryTemplate1(query[1:4], result), queryTemplate2(query[4:6], result), how='inner')\n",
    "    elif (query[0] == '2or2'):\n",
    "        result = result.append(queryTemplate2(query[1:3], result))\n",
    "        result = result.append(queryTemplate2(query[3:5], result))\n",
    "    elif (query[0] == '2and2'):\n",
    "        result = pd.merge(queryTemplate2(query[1:3], result), queryTemplate2(query[3:5], result), how='inner')\n",
    "    return result.drop_duplicates()\n",
    "\n",
    "while(True):\n",
    "    result = pd.DataFrame(data=None, columns=association_rules.columns)\n",
    "    query = input('Enter new query: ')\n",
    "    if(query == \"exit\"):\n",
    "        break\n",
    "    elif (query.startswith('asso_rule.template1')):\n",
    "        query= query.strip('asso_rule.template1')\n",
    "        query = eval(query)\n",
    "        result = queryTemplate1(query, result)\n",
    "    elif (query.startswith('asso_rule.template2')):\n",
    "        query= query.strip('asso_rule.template2')\n",
    "        query = eval(query)\n",
    "        result = queryTemplate2(query, result)\n",
    "    elif (query.startswith('asso_rule.template3')):\n",
    "        query= query.strip('asso_rule.template3')\n",
    "        query = eval(query)\n",
    "        result = queryTemplate3(query, result)\n",
    "    else:\n",
    "        print('Invalid query. \\n')\n",
    "        continue\n",
    "    print(str(len(result)) + \" rules are generated for the given query.\\n\")\n",
    "#     print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
